`   # -*- coding: utf-8 -*-
"""RNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N5oQlKFFN3YfYgRiciAwMmMCvEHbt9Xe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('Frozen_Dessert_Production.csv',parse_dates=True,index_col=['DATE'])
df.head()

df.info()

df.columns = ['Productions']
df.head()

df.head()

df.plot()

len(df)

test_size = 24
train_ind = len(df) - test_size

train_ind

test_size

train = df.iloc[:train_ind]
test = df.iloc[train_ind:]

train

test

len(test)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

train_scale = scaler.fit_transform(train)
test_scale = scaler.transform(test)

length = 23
batch_size = 1
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
generator = TimeseriesGenerator(train_scale,train_scale,length=length,
                                batch_size=1)

v,w = generator[0]
w

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import LSTM,Dense
n_features=1
model = Sequential()

model = Sequential()
model.add(LSTM(128,activation='tanh',input_shape=(length,n_features)))
model.add(Dense(1))
model.compile(optimizer='adam',loss='mse')

model.summary()
validation_generator = TimeseriesGenerator(test_scale,test_scale,length=length,
                                           batch_size=1)

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss',patience=3)

model.fit_generator(generator,epochs=20,callbacks=[early_stop],
                    validation_data=validation_generator)

loss = pd.DataFrame(model.history.history)

loss.plot()

test_predictions = []
first_eval_batch = train_scale[-length:]
current_batch = first_eval_batch.reshape(1,length,n_features)
for i in range(len(test)):
  current_pred = model.predict(current_batch)[0]
  test_predictions.append(current_pred)
  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

test_predictions

true_predictions = scaler.inverse_transform(test_predictions)
true_predictions

import warnings
warnings.filterwarnings('ignore')

test['test_predictions'] = true_predictions
test.plot()

full_scaled_train = scaler.fit_transform(df)

model = Sequential()
model.add(LSTM(128,activation='tanh',input_shape=(length,n_features)))
model.add(Dense(1))
model.compile(optimizer='adam',loss='mse')

model.summary()
generator = TimeseriesGenerator(full_scaled_train,full_scaled_train,length=length,
                                           batch_size=1)

model.fit_generator(generator,epochs=8)

loses = pd.DataFrame(model.history.history)
loses.plot()

forecaste = []
periods = 240
first_eval_batch = train_scale[-length:]
current_batch = first_eval_batch.reshape(1,length,n_features)
for i in range(periods):
  current_pred = model.predict(current_batch)[0]
  forecaste.append(current_pred)
  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

10*24

forecaste

true_forecaste = scaler.inverse_transform(forecaste)
true_forecaste

df

df_index = pd.date_range('2019-10-01',periods=periods,freq='MS')
len(df_index)

forecaste_pred_df = pd.DataFrame(data=true_forecaste,index=df_index,columns=['Forescaste_pred'])
forecaste_pred_df

ax = df.plot()
forecaste_pred_df.plot(ax=ax)
plt.xlim('2018-01-01','2022-01-01')

